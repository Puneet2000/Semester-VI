{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7 : Game of Tic-Tac-Toe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "        \n",
    "class TicTacToe(object):\n",
    "    \"\"\"A grid world with pellets to collect and an enemy to avoid.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Environments have fixed size and pellet counts.\"\"\"\n",
    "        self.board = np.zeros((3,3))\n",
    "        self.isEnd = False\n",
    "        self.next_reward = 0\n",
    "    \n",
    "    def initialise(self):\n",
    "        self.board = np.zeros((3,3))\n",
    "        self.isEnd = False\n",
    "        self.next_reward = 0\n",
    "        return random.choice([1,-1])\n",
    "\n",
    "    def actions(self):\n",
    "        positions = []\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                if self.board[i, j] == 0:\n",
    "                    positions.append((i, j))  # need to be tuple\n",
    "        return positions\n",
    "\n",
    "    def reward(self):\n",
    "        result = self.winner()\n",
    "        # backpropagate reward\n",
    "        if result == 1:\n",
    "            return  10\n",
    "        elif result == -1:\n",
    "            return -10\n",
    "        elif result == 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def winner(self):\n",
    "        # row\n",
    "        for i in range(3):\n",
    "            if sum(self.board[i, :]) == 3:\n",
    "                self.isEnd = True\n",
    "                return 1\n",
    "            if sum(self.board[i, :]) == -3:\n",
    "                self.isEnd = True\n",
    "                return -1\n",
    "        # col\n",
    "        for i in range(3):\n",
    "            if sum(self.board[:, i]) == 3:\n",
    "                self.isEnd = True\n",
    "                return 1\n",
    "            if sum(self.board[:, i]) == -3:\n",
    "                self.isEnd = True\n",
    "                return -1\n",
    "        # diagonal\n",
    "        diag_sum1 = sum([self.board[i, i] for i in range(3)])\n",
    "        diag_sum2 = sum([self.board[i, 3 - i - 1] for i in range(3)])\n",
    "        diag_sum = max(abs(diag_sum1), abs(diag_sum2))\n",
    "        if diag_sum == 3:\n",
    "            self.isEnd = True\n",
    "            if diag_sum1 == 3 or diag_sum2 == 3:\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "\n",
    "        # tie\n",
    "        # no available positions\n",
    "        if len(self.actions()) == 0:\n",
    "            self.isEnd = True\n",
    "            return 0\n",
    "        # not end\n",
    "        self.isEnd = False\n",
    "        return None\n",
    "\n",
    "    def act(self, position, symbol):\n",
    "        self.board[position] = symbol\n",
    "        \n",
    "\n",
    "    def state(self):\n",
    "        return str(self.board.reshape(3*3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent_Random(object):\n",
    "    \"\"\"Learns to act random within the environment.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def policy(self, s, actions):\n",
    "        return random.choice(actions)\n",
    "\n",
    "class Agent_Safe(object):\n",
    "    \"\"\"Learns to act safe within the environment.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def policy(self, board, actions):\n",
    "        # row\n",
    "        for i in range(3):\n",
    "            if sum(board[i, :]) == 2 or sum(board[i, :]) == -2:\n",
    "                return (i,list(board[i,:]).index(0))\n",
    "        # col\n",
    "        for i in range(3):\n",
    "            if sum(board[:, i]) == 2 or sum(board[:, i]) == -2:\n",
    "                return (list(board[:,i]).index(0),i)\n",
    "            \n",
    "        # diagonal\n",
    "        diag1 = [board[i, i] for i in range(3)]\n",
    "        diag2 = [board[i, 3 - i - 1] for i in range(3)]\n",
    "\n",
    "        if sum(diag1) == 2 or sum(diag1) == -2:\n",
    "                return (diag1.index(0),diag1.index(0))\n",
    "        \n",
    "        if sum(diag2) == 2 or sum(diag2) == -2:\n",
    "                return (diag2.index(0),diag2.index(0))\n",
    "        \n",
    "        return random.choice(actions)\n",
    "        \n",
    "        \n",
    "\n",
    "class Agent_Qlearn(object):\n",
    "    \"\"\"Learns to act within the environment.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epsilon = 0.05 # Exploration rate\n",
    "        self.gamma = 0.9 # Discount factor\n",
    "        self.alpha = 0.01 # Learning rate\n",
    "        self.Q_values = {}\n",
    "\n",
    "    def choose(self, s, actions):\n",
    "        \"\"\"Return an action to try in this state.\"\"\"\n",
    "        p = random.random()\n",
    "        if p < self.epsilon:\n",
    "            return random.choice(actions)\n",
    "        else:\n",
    "            return self.policy(s, actions)\n",
    "\n",
    "    def policy(self, s, actions):\n",
    "        \"\"\"Return the best action for this state.\"\"\"\n",
    "        max_value = max([self.Q(s,a) for a in actions])\n",
    "        max_actions = [a for a in actions if self.Q(s,a) == max_value]\n",
    "        return random.choice(max_actions)\n",
    "\n",
    "    def Q(self, s, a):\n",
    "        \"\"\"Return the estimated Q-value of this action in this state.\"\"\"\n",
    "        if (s,a) not in self.Q_values:\n",
    "            self.Q_values[(s,a)] = 0\n",
    "        return self.Q_values[(s,a)]\n",
    "    \n",
    "    def observe(self, s, a, sp, r, actions):\n",
    "        \"\"\"Update weights based on this observed step.\"\"\"\n",
    "        if len(actions) == 0:\n",
    "            max_value = 0\n",
    "        else:\n",
    "            max_value = max([self.Q(sp,a) for a in actions])\n",
    "        self.Q_values[(s,a)] = (1-self.alpha)*self.Q(s,a) + self.alpha*(r + self.gamma*max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(environment,agent,opponent,rounds=100):\n",
    "    rewards = []\n",
    "    for epoch in range(rounds):\n",
    "        turn = environment.initialise()\n",
    "        if turn == -1:\n",
    "            actions = environment.actions()\n",
    "            a_ = opponent.policy(environment.board,actions)\n",
    "            environment.act(a_,-1)\n",
    "        while not environment.isEnd:   \n",
    "            s = environment.state()\n",
    "            actions = environment.actions()\n",
    "            a = agent.policy(s, actions)   \n",
    "            environment.act(a,1)\n",
    "\n",
    "            win = environment.winner()\n",
    "            if win is not None:\n",
    "                r = environment.reward()\n",
    "            else:\n",
    "                actions = environment.actions()\n",
    "                a_ = opponent.policy(environment.board,actions)\n",
    "                environment.act(a_,-1)\n",
    "                r = environment.reward()\n",
    "        rewards.append(r)\n",
    "    print('Wins', rewards.count(10),'Loses', rewards.count(-10), 'Draws', rewards.count(1))\n",
    "\n",
    "def train(environment,agent,opponents):\n",
    "    for epoch in range(10000):\n",
    "        opponent = random.choice(opponents)\n",
    "        turn = environment.initialise()\n",
    "        if turn == -1:\n",
    "            actions = environment.actions()\n",
    "            a_ = opponent.policy(environment.board,actions)\n",
    "            environment.act(a_,-1)\n",
    "        while not environment.isEnd:   \n",
    "            s = environment.state()\n",
    "            actions = environment.actions()\n",
    "            a = agent.choose(s, actions)   \n",
    "            environment.act(a,1)\n",
    "    #         print(environment.board)\n",
    "            sp = environment.state()\n",
    "            win = environment.winner()\n",
    "            if win is not None:\n",
    "                r = environment.reward()\n",
    "            else:\n",
    "                actions = environment.actions()\n",
    "                a_ = opponent.policy(environment.board,actions)\n",
    "                environment.act(a_,-1)\n",
    "    #             print(environment.board)\n",
    "                sp = environment.state()\n",
    "                r = environment.reward()\n",
    "            actions = environment.actions()\n",
    "            agent.observe(s,a,sp,r,actions)\n",
    "        if epoch%200 == 0:\n",
    "            print('Epoch',epoch,end=' ')\n",
    "            test(environment,agent,opponent)\n",
    "    return agent\n",
    "\n",
    "environment = TicTacToe()\n",
    "opponent1 = Agent_Random()\n",
    "agent_q1 = Agent_Qlearn()\n",
    "\n",
    "opponent2 = Agent_Safe()\n",
    "agent_q2 = Agent_Qlearn()\n",
    "\n",
    "agent_q3 = Agent_Qlearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Wins 45 Loses 44 Draws 11\n",
      "Epoch 200 Wins 53 Loses 39 Draws 8\n",
      "Epoch 400 Wins 56 Loses 35 Draws 9\n",
      "Epoch 600 Wins 54 Loses 34 Draws 12\n",
      "Epoch 800 Wins 62 Loses 25 Draws 13\n",
      "Epoch 1000 Wins 69 Loses 20 Draws 11\n",
      "Epoch 1200 Wins 67 Loses 25 Draws 8\n",
      "Epoch 1400 Wins 72 Loses 24 Draws 4\n",
      "Epoch 1600 Wins 75 Loses 19 Draws 6\n",
      "Epoch 1800 Wins 71 Loses 20 Draws 9\n",
      "Epoch 2000 Wins 75 Loses 14 Draws 11\n",
      "Epoch 2200 Wins 79 Loses 14 Draws 7\n",
      "Epoch 2400 Wins 85 Loses 10 Draws 5\n",
      "Epoch 2600 Wins 79 Loses 12 Draws 9\n",
      "Epoch 2800 Wins 80 Loses 8 Draws 12\n",
      "Epoch 3000 Wins 82 Loses 8 Draws 10\n",
      "Epoch 3200 Wins 75 Loses 8 Draws 17\n",
      "Epoch 3400 Wins 80 Loses 6 Draws 14\n",
      "Epoch 3600 Wins 81 Loses 7 Draws 12\n",
      "Epoch 3800 Wins 71 Loses 6 Draws 23\n",
      "Epoch 4000 Wins 85 Loses 5 Draws 10\n",
      "Epoch 4200 Wins 79 Loses 5 Draws 16\n",
      "Epoch 4400 Wins 85 Loses 4 Draws 11\n",
      "Epoch 4600 Wins 88 Loses 4 Draws 8\n",
      "Epoch 4800 Wins 86 Loses 4 Draws 10\n",
      "Epoch 5000 Wins 83 Loses 3 Draws 14\n",
      "Epoch 5200 Wins 80 Loses 5 Draws 15\n",
      "Epoch 5400 Wins 76 Loses 3 Draws 21\n",
      "Epoch 5600 Wins 76 Loses 9 Draws 15\n",
      "Epoch 5800 Wins 82 Loses 5 Draws 13\n",
      "Epoch 6000 Wins 80 Loses 3 Draws 17\n",
      "Epoch 6200 Wins 84 Loses 2 Draws 14\n",
      "Epoch 6400 Wins 87 Loses 3 Draws 10\n",
      "Epoch 6600 Wins 86 Loses 3 Draws 11\n",
      "Epoch 6800 Wins 86 Loses 5 Draws 9\n",
      "Epoch 7000 Wins 85 Loses 2 Draws 13\n",
      "Epoch 7200 Wins 75 Loses 6 Draws 19\n",
      "Epoch 7400 Wins 88 Loses 3 Draws 9\n",
      "Epoch 7600 Wins 85 Loses 2 Draws 13\n",
      "Epoch 7800 Wins 83 Loses 5 Draws 12\n",
      "Epoch 8000 Wins 86 Loses 5 Draws 9\n",
      "Epoch 8200 Wins 87 Loses 2 Draws 11\n",
      "Epoch 8400 Wins 83 Loses 2 Draws 15\n",
      "Epoch 8600 Wins 80 Loses 5 Draws 15\n",
      "Epoch 8800 Wins 86 Loses 5 Draws 9\n",
      "Epoch 9000 Wins 77 Loses 5 Draws 18\n",
      "Epoch 9200 Wins 77 Loses 4 Draws 19\n",
      "Epoch 9400 Wins 84 Loses 2 Draws 14\n",
      "Epoch 9600 Wins 83 Loses 2 Draws 15\n",
      "Epoch 9800 Wins 87 Loses 2 Draws 11\n"
     ]
    }
   ],
   "source": [
    "#1 Training against random agent\n",
    "agent_q1 = train(environment,agent_q1,[opponent1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Wins 13 Loses 72 Draws 15\n",
      "Epoch 200 Wins 11 Loses 66 Draws 23\n",
      "Epoch 400 Wins 17 Loses 60 Draws 23\n",
      "Epoch 600 Wins 8 Loses 59 Draws 33\n",
      "Epoch 800 Wins 27 Loses 39 Draws 34\n",
      "Epoch 1000 Wins 27 Loses 32 Draws 41\n",
      "Epoch 1200 Wins 31 Loses 32 Draws 37\n",
      "Epoch 1400 Wins 29 Loses 29 Draws 42\n",
      "Epoch 1600 Wins 49 Loses 16 Draws 35\n",
      "Epoch 1800 Wins 33 Loses 22 Draws 45\n",
      "Epoch 2000 Wins 45 Loses 14 Draws 41\n",
      "Epoch 2200 Wins 34 Loses 15 Draws 51\n",
      "Epoch 2400 Wins 40 Loses 13 Draws 47\n",
      "Epoch 2600 Wins 35 Loses 11 Draws 54\n",
      "Epoch 2800 Wins 38 Loses 10 Draws 52\n",
      "Epoch 3000 Wins 37 Loses 6 Draws 57\n",
      "Epoch 3200 Wins 45 Loses 6 Draws 49\n",
      "Epoch 3400 Wins 34 Loses 8 Draws 58\n",
      "Epoch 3600 Wins 44 Loses 6 Draws 50\n",
      "Epoch 3800 Wins 38 Loses 1 Draws 61\n",
      "Epoch 4000 Wins 43 Loses 5 Draws 52\n",
      "Epoch 4200 Wins 42 Loses 3 Draws 55\n",
      "Epoch 4400 Wins 42 Loses 6 Draws 52\n",
      "Epoch 4600 Wins 39 Loses 4 Draws 57\n",
      "Epoch 4800 Wins 38 Loses 1 Draws 61\n",
      "Epoch 5000 Wins 34 Loses 5 Draws 61\n",
      "Epoch 5200 Wins 43 Loses 5 Draws 52\n",
      "Epoch 5400 Wins 47 Loses 2 Draws 51\n",
      "Epoch 5600 Wins 41 Loses 3 Draws 56\n",
      "Epoch 5800 Wins 50 Loses 2 Draws 48\n",
      "Epoch 6000 Wins 43 Loses 2 Draws 55\n",
      "Epoch 6200 Wins 53 Loses 2 Draws 45\n",
      "Epoch 6400 Wins 39 Loses 3 Draws 58\n",
      "Epoch 6600 Wins 33 Loses 4 Draws 63\n",
      "Epoch 6800 Wins 44 Loses 1 Draws 55\n",
      "Epoch 7000 Wins 40 Loses 3 Draws 57\n",
      "Epoch 7200 Wins 37 Loses 1 Draws 62\n",
      "Epoch 7400 Wins 52 Loses 1 Draws 47\n",
      "Epoch 7600 Wins 42 Loses 1 Draws 57\n",
      "Epoch 7800 Wins 32 Loses 3 Draws 65\n",
      "Epoch 8000 Wins 35 Loses 4 Draws 61\n",
      "Epoch 8200 Wins 41 Loses 2 Draws 57\n",
      "Epoch 8400 Wins 35 Loses 1 Draws 64\n",
      "Epoch 8600 Wins 37 Loses 1 Draws 62\n",
      "Epoch 8800 Wins 36 Loses 6 Draws 58\n",
      "Epoch 9000 Wins 57 Loses 0 Draws 43\n",
      "Epoch 9200 Wins 38 Loses 1 Draws 61\n",
      "Epoch 9400 Wins 48 Loses 0 Draws 52\n",
      "Epoch 9600 Wins 34 Loses 2 Draws 64\n",
      "Epoch 9800 Wins 43 Loses 2 Draws 55\n"
     ]
    }
   ],
   "source": [
    "#2 Training against safe agent\n",
    "agent_q2 = train(environment,agent_q2,[opponent2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Wins 45 Loses 44 Draws 11\n",
      "Epoch 200 Wins 15 Loses 68 Draws 17\n",
      "Epoch 400 Wins 47 Loses 37 Draws 16\n",
      "Epoch 600 Wins 14 Loses 67 Draws 19\n",
      "Epoch 800 Wins 20 Loses 54 Draws 26\n",
      "Epoch 1000 Wins 30 Loses 54 Draws 16\n",
      "Epoch 1200 Wins 44 Loses 43 Draws 13\n",
      "Epoch 1400 Wins 62 Loses 24 Draws 14\n",
      "Epoch 1600 Wins 61 Loses 28 Draws 11\n",
      "Epoch 1800 Wins 61 Loses 27 Draws 12\n",
      "Epoch 2000 Wins 72 Loses 20 Draws 8\n",
      "Epoch 2200 Wins 62 Loses 15 Draws 23\n",
      "Epoch 2400 Wins 53 Loses 19 Draws 28\n",
      "Epoch 2600 Wins 60 Loses 20 Draws 20\n",
      "Epoch 2800 Wins 77 Loses 10 Draws 13\n",
      "Epoch 3000 Wins 74 Loses 11 Draws 15\n",
      "Epoch 3200 Wins 53 Loses 17 Draws 30\n",
      "Epoch 3400 Wins 75 Loses 9 Draws 16\n",
      "Epoch 3600 Wins 60 Loses 6 Draws 34\n",
      "Epoch 3800 Wins 69 Loses 8 Draws 23\n",
      "Epoch 4000 Wins 81 Loses 7 Draws 12\n",
      "Epoch 4200 Wins 69 Loses 6 Draws 25\n",
      "Epoch 4400 Wins 67 Loses 4 Draws 29\n",
      "Epoch 4600 Wins 61 Loses 5 Draws 34\n",
      "Epoch 4800 Wins 68 Loses 7 Draws 25\n",
      "Epoch 5000 Wins 67 Loses 6 Draws 27\n",
      "Epoch 5200 Wins 76 Loses 4 Draws 20\n",
      "Epoch 5400 Wins 75 Loses 9 Draws 16\n",
      "Epoch 5600 Wins 69 Loses 5 Draws 26\n",
      "Epoch 5800 Wins 71 Loses 3 Draws 26\n",
      "Epoch 6000 Wins 71 Loses 3 Draws 26\n",
      "Epoch 6200 Wins 69 Loses 13 Draws 18\n",
      "Epoch 6400 Wins 60 Loses 8 Draws 32\n",
      "Epoch 6600 Wins 73 Loses 7 Draws 20\n",
      "Epoch 6800 Wins 80 Loses 5 Draws 15\n",
      "Epoch 7000 Wins 64 Loses 11 Draws 25\n",
      "Epoch 7200 Wins 62 Loses 7 Draws 31\n",
      "Epoch 7400 Wins 65 Loses 3 Draws 32\n",
      "Epoch 7600 Wins 84 Loses 3 Draws 13\n",
      "Epoch 7800 Wins 69 Loses 3 Draws 28\n",
      "Epoch 8000 Wins 77 Loses 4 Draws 19\n",
      "Epoch 8200 Wins 83 Loses 4 Draws 13\n",
      "Epoch 8400 Wins 68 Loses 5 Draws 27\n",
      "Epoch 8600 Wins 77 Loses 4 Draws 19\n",
      "Epoch 8800 Wins 59 Loses 5 Draws 36\n",
      "Epoch 9000 Wins 63 Loses 4 Draws 33\n",
      "Epoch 9200 Wins 83 Loses 0 Draws 17\n",
      "Epoch 9400 Wins 84 Loses 4 Draws 12\n",
      "Epoch 9600 Wins 83 Loses 2 Draws 15\n",
      "Epoch 9800 Wins 92 Loses 1 Draws 7\n"
     ]
    }
   ],
   "source": [
    "#3 Training against both agent randomly\n",
    "agent_q3 = train(environment,agent_q3,[opponent1,opponent2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent train against random vs random\n",
      "Wins 845 Loses 24 Draws 131\n",
      "Agent train against random vs safe\n",
      "Wins 397 Loses 175 Draws 428\n",
      "Agent train against safe vs random\n",
      "Wins 531 Loses 229 Draws 240\n",
      "Agent train against safe vs safe\n",
      "Wins 449 Loses 18 Draws 533\n",
      "Agent train against both vs random\n",
      "Wins 831 Loses 32 Draws 137\n",
      "Agent train against both vs safe\n",
      "Wins 632 Loses 27 Draws 341\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "print('Agent train against random vs random')\n",
    "test(environment, agent_q1,opponent1,1000)\n",
    "print('Agent train against random vs safe')\n",
    "test(environment, agent_q1,opponent2,1000)\n",
    "print('Agent train against safe vs random')\n",
    "test(environment, agent_q2,opponent1,1000)\n",
    "print('Agent train against safe vs safe')\n",
    "test(environment, agent_q2,opponent2,1000)\n",
    "print('Agent train against both vs random')\n",
    "test(environment, agent_q3,opponent1,1000)\n",
    "print('Agent train against both vs safe')\n",
    "test(environment, agent_q3,opponent2,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Among the three, the third agent which was trained against both performs relatively best. As it now is trained with safe moves and random moves too. More like Ensembling\n",
    "\n",
    "(e) No the agents are not unbeatable to aany opponent. The training can be improved by optimizing hyperparameters- Training iterations or Training the robot against itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
