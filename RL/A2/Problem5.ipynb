{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5 : Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s1', 'a1', 1, 's1', 'a2', 2, 's2'] 0 0\n",
      "['s1', 'a1', 1, 's1', 'a2', 2, 's2'] 0 0\n",
      "['s1', 'a1', 1, 's1', 'a2', 2, 's2'] 0 0\n",
      "['s1', 'a1', 1, 's1', 'a2', 2, 's2'] 0 0\n",
      "['s1', 'a1', 1, 's1', 'a2', 2, 's2'] 0 0\n",
      "['s1', 'a1', 1, 's1', 'a2', 2, 's2'] 0 0\n",
      "['s1', 'a1', 1, 's1', 'a2', 2, 's2'] 0 0\n",
      "['s1', 'a1', 1, 's1', 'a2', 2, 's2'] 0 0\n",
      "['s1', 'a1', 1, 's1', 'a2', 2, 's2'] 0 0\n",
      "['s1', 'a1', 1, 's1', 'a2', 2, 's2'] 0 0\n"
     ]
    }
   ],
   "source": [
    "environment = {\n",
    "        'a1': ('s1',1),\n",
    "        'a2': ('s2',2),\n",
    "        'a3': ('s3',3)\n",
    "}\n",
    "\n",
    "class Agent(object):\n",
    "    \"\"\"Learns to act within the environment.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epsilon = 0.3 # Exploration rate\n",
    "        self.gamma = 0.5 # Discount factor\n",
    "        self.alpha = 0.7 # Learning rate\n",
    "        self.Q_values = {}\n",
    "        \n",
    "    def choose(self, s, actions):\n",
    "        \"\"\"Return an action to try in this state.\"\"\"\n",
    "        p = random.random()\n",
    "        if p < self.epsilon:\n",
    "            return 0,random.choice(actions)\n",
    "        else:\n",
    "            return 1,self.policy(s, actions)\n",
    "\n",
    "    def policy(self, s, actions):\n",
    "        \"\"\"Return the best action for this state.\"\"\"\n",
    "        max_value = max([self.Q(s,a) for a in actions])\n",
    "        max_actions = [a for a in actions if self.Q(s,a) == max_value]\n",
    "        return max_actions[0]\n",
    "\n",
    "    def Q(self, s, a):\n",
    "        \"\"\"Return the estimated Q-value of this action in this state.\"\"\"\n",
    "        if (s,a) not in self.Q_values:\n",
    "            self.Q_values[(s,a)] = 0\n",
    "        return self.Q_values[(s,a)]\n",
    "    \n",
    "    def observe(self, s, a, sp, r, actions):\n",
    "        \"\"\"Update weights based on this observed step.\"\"\"\n",
    "        max_value = max([self.Q(sp,a) for a in actions])\n",
    "        self.Q_values[(s,a)] = (1-self.alpha)*self.Q(s,a) + self.alpha*(r + self.gamma*max_value)\n",
    "\n",
    "agent = Agent()\n",
    "states = ['s1','s2','s3']\n",
    "actions = ['a1','a2','a3']\n",
    "wanted  = ['s1','a1',1,'s1','a2',2,'s2']\n",
    "\n",
    "for epoch in range(1000):\n",
    "    s = random.choice(states)\n",
    "    for _ in range(3):\n",
    "        _,a = agent.choose(s, actions)\n",
    "        (sp,r) = environment[a]\n",
    "        agent.observe(s, a, sp, r, actions)\n",
    "        s = sp\n",
    "\n",
    "for epoch in range(1000):\n",
    "    s1 = 's1'\n",
    "    c1,a1 = agent.choose(s1, actions)\n",
    "    (s2,r1) = environment[a1]\n",
    "    \n",
    "    c2,a2 = agent.choose(s2, actions)\n",
    "    (s3,r2) = environment[a2]\n",
    "    \n",
    "    seq = [s1,a1,r1,s2,a2,r2,s3]\n",
    "    if seq == wanted:\n",
    "        print(seq,c1,c2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, we obtained the sequence, as shown above. Both actions chosen were random and not from greedy policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
